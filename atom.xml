<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zekang Li&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lizekang.github.io/"/>
  <updated>2018-11-20T12:18:56.248Z</updated>
  <id>https://lizekang.github.io/</id>
  
  <author>
    <name>Zekang Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Evaluation metrics for generation based models in NLP</title>
    <link href="https://lizekang.github.io/2018/11/20/Evaluation-metrics-for-generation-based-models-in-NLP/"/>
    <id>https://lizekang.github.io/2018/11/20/Evaluation-metrics-for-generation-based-models-in-NLP/</id>
    <published>2018-11-20T12:16:45.000Z</published>
    <updated>2018-11-20T12:18:56.248Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Evaluation-metrics-for-generation-based-models-in-NLP"><a href="#Evaluation-metrics-for-generation-based-models-in-NLP" class="headerlink" title="Evaluation metrics for generation based models in NLP"></a>Evaluation metrics for generation based models in NLP</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>转自： <a href="http://www.cnblogs.com/by-dream/p/7679284.html" target="_blank" rel="noopener">http://www.cnblogs.com/by-dream/p/7679284.html</a> </p><h4 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a>n-gram</h4><p>BLEU 采用一种N-gram的匹配规则，原理比较简单，就是比较译文和参考译文之间n组词的相似的一个占比。</p><p>　　例如：</p><p>　　　　原文：今天天气不错</p><p>　　　　机器译文：It is a nice day today</p><p>　　　　人工译文：Today is a nice day</p><p>　　如果用1-gram匹配的话：</p><p>　　<img src="/images/BLEU1.png" alt="BLEU1"></p><p>　　可以看到机器译文一共6个词，有5个词语都命中的了参考译文，那么它1-gram的匹配度为 5/6 </p><p>　　我们再以3-gram举例：</p><p>​    <img src="/images/BLEU2.png" alt="BLEU2"></p><p>　　可以看到机器译文一共可以分为四个3-gram的词组，其中有两个可以命中参考译文，那么它3-gram的匹配度为 2/4  </p><p>　　依次类推，我们可以很容易实现一个程序来遍历计算N-gram的一个匹配度。一般来说1-gram的结果代表了文中有多少个词被单独翻译出来了，因此它反映的是这篇译文的忠实度；而当我们计算2-gram以上时，更多时候结果反映的是译文的流畅度，值越高文章的可读性就越好。</p><h4 id="召回率修正"><a href="#召回率修正" class="headerlink" title="召回率修正"></a>召回率修正</h4><p>面所说的方法比较好理解，也比较好实现，但是没有考虑到召回率，举一个非常简单的例子说明：</p><p>　　原文：猫站在地上</p><p>　　机器译文：the the the the </p><p>　　人工译文：The cat is standing on the ground</p><p>在计算1-gram的时候，the 都出现在译文中，因此匹配度为4/4 ，但是很明显 the 在人工译文中最多出现的次数只有2次，因此BLEU算法修正了这个值的算法，首先会计算该n-gram在译文中可能出现的最大次数：</p><p>$$Count_{clip}=\min(Count, Max_ref_count)$$</p><p>Count是N-gram在机器翻译译文中的出现次数，$Max_Ref_Count$是该N-gram在一个参考译文中最大的出现次数，最终统计结果取两者中的较小值。然后在把这个匹配结果除以机器翻译译文的N-gram个数。因此对于上面的例子来说，修正后的1-gram的统计结果就是2/4。</p><p>我们将整个要处理的将机器翻译的句子表示为$C_{i}$，标准答案表示为 $S_{i}=s_{i1},…,s_{im}$（m表示有m个参考答案）　　</p><p>　　n-grams表示n个单词长度的词组集合，令$W_{k}$第k个n-gram</p><p>　　比如这样的一句话，”I come from china”，<em>第1个2-gram为：I come; 第2个2-gram为：come from; 第3个2-gram为：from china;</em></p><p>　　$H_{k}(C_{i})$ 表示$W_{k}$翻译选译文$C_{i}$中出现的次数</p><p>　　$H_{k}(S_{ij})$ 表示$W_{k}$在标准答案$S_{ij}$中出现的次数</p><p>所以各阶n-gram的精度可以用下面的公式计算：</p><p>$$P_{n} = \frac{\sum_{i}\sum_{k}\min(H_{k}(C_{i}, max_{j\in m}H_{k}(S_{ij})))}{\sum_{i}\sum_{k}min(H_{k}(C_{i}))}$$</p><h4 id="惩罚因子"><a href="#惩罚因子" class="headerlink" title="惩罚因子"></a>惩罚因子</h4><p>　　上面的算法已经足够可以有效的翻译评估了，然而N-gram的匹配度可能会随着句子长度的变短而变好，因此会存在这样一个问题：一个翻译引擎只翻译出了句子中部分句子且翻译的比较准确，那么它的匹配度依然会很高。为了避免这种评分的偏向性，BLEU在最后的评分结果中引入了长度惩罚因子(Brevity Penalty)。</p><p><img src="/images/BLEU4.png" alt="BLEU4"></p><p>$l_{c}$代表表示机器翻译译文的长度，$l_{s}$表示参考答案的有效长度，当存在多个参考译文时，选取和翻译译文最接近的长度。当翻译译文长度大于参考译文的长度时，惩罚系数为1，意味着不惩罚，只有机器翻译译文长度小于参考答案才会计算惩罚因子。</p><h4 id="BLEU-1"><a href="#BLEU-1" class="headerlink" title="BLEU"></a>BLEU</h4><p>由于各N-gram统计量的精度随着阶数的升高而呈指数形式递减，所以为了平衡各阶统计量的作用，对其采用几何平均形式求平均值然后加权，再乘以长度惩罚因子，得到最后的评价公式：</p><p>$$BLEU = BP× exp(\sum_{n=1}^{N}W_{n}logP_{n})$$</p><p>其中，$W_{n}=1/n$，N的取值最大为4。</p><h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h3><blockquote><p>Rouge(Recall-Oriented Understudy for Gisting Evaluation) 一种基于召回率的相似性度量方法。</p></blockquote><h4 id="ROUGE-N"><a href="#ROUGE-N" class="headerlink" title="ROUGE-N"></a>ROUGE-N</h4><p>$$ROUGE-N = \frac{\sum_{S\in{ReferencesSummaries}\sum_{gram_{n}\in S}Count_{match}(gram_{n})}}{\sum_{S\in{ReferencesSummaries}\sum_{gram_{n}\in S}Count(gram_{n})}}$$</p><p>n代表n元组，$Count_{match}(gram_{n})$是待评测句子中出现的最大匹配的n-grams的个数，从分子中可以看出ROUGE-N是一个基于召回率的指标</p><h4 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h4><p>L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。</p><p>$$R_{cls} = \frac{LCS(X,Y)}{m}$$</p><p>$$P_{cls}=\frac{LCS(X,Y)}{n}$$</p><p>$$F_{cls}=\frac{(1+\beta^{2}R_{cls}P_{cls})}{R_{cls}+\beta^{2}P_{cls}}$$</p><p>其中$LCS(X, Y)$是X和Y的最长公共子序列的长度，m，n分别表示参考摘要和自动摘要的长度$R_{cls}$和$P_{cls}$分别表示召回率和率，$精确F_{cls}$就是我们的ROUGE-L</p><table><thead><tr><th>ROUGE-N</th><th>基于n-gram共现性统计</th></tr></thead><tbody><tr><td>ROUGE-L</td><td>基于最长共有子序列共现性精确度和召回率Fmeasure统计</td></tr><tr><td>ROUGE-W</td><td>带权重的最长共有子序列共现性精确度和召回率Fmeasure统计</td></tr><tr><td>ROUGE-S</td><td>不连续二元组共现性精确度和召回率Fmeasure统计</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Evaluation-metrics-for-generation-based-models-in-NLP&quot;&gt;&lt;a href=&quot;#Evaluation-metrics-for-generation-based-models-in-NLP&quot; class=&quot;heade
      
    
    </summary>
    
      <category term="NLP" scheme="https://lizekang.github.io/categories/NLP/"/>
    
    
      <category term="metrics" scheme="https://lizekang.github.io/tags/metrics/"/>
    
  </entry>
  
  <entry>
    <title>年度爆款BERT!</title>
    <link href="https://lizekang.github.io/2018/11/19/%E5%B9%B4%E5%BA%A6%E7%88%86%E6%AC%BEBERT/"/>
    <id>https://lizekang.github.io/2018/11/19/年度爆款BERT/</id>
    <published>2018-11-19T09:37:23.000Z</published>
    <updated>2018-11-20T03:11:52.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding》阅读笔记"><a href="#《BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding》阅读笔记" class="headerlink" title="《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》阅读笔记"></a>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》阅读笔记</h2><p>BERT算是今年的一个爆款了，同一个模型，只微调就刷新了十一个记录，都有很大的提升…<br>它的出现为之后NLP的发展打下了很好的基础<br>下面来看论文中的做法</p><h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><p>论文中介绍了预训练的各种方法:</p><ul><li>Feature-based Approaches (ELMo)</li><li>Fine-tuning Approaches (OpenAI GPT， BERT)</li><li>Transfer Learning from Supervised Data<br><img src="/images/bert1.png" alt="image"><br>论文中介绍了BERT与其他各种预训练模型ELMo和OpenAI GPT的区别，ELMo使用了双向LSTM提取特征，属于feature-based方法。给downstream的任务生成features。OpenAI GPT只使用了left-to-right的Transformer，而token的right方向的信息是获取不到的。而BERT在所有层中都融合left-to-right和right-to-left的信息。</li></ul><h3 id="Input-Representations"><a href="#Input-Representations" class="headerlink" title="Input Representations"></a>Input Representations</h3><p><img src="/images/bert2.png" alt="image"><br>BERT简单粗暴的将句子拼接在一起（无论是单句还是句子对，连在一块就完事了），它的Input Representations 如图</p><ul><li>Token embedding 用了WordPiece embedding，这个方法是用来解决oov的word的，就是把一些单词拆成两个词，比如playing就可以拆成play和##ing，拆的时候就用贪心去搜索最少的token来覆盖所有的单词</li><li>Segment embedding 区分一下句子一和二</li><li>Position embedding 融入了位置信息，是通过模型学到的</li><li>The input embeddings is the sum of the token embeddings, the segmentation embeddings and the position embeddings. 没错，就这么粗暴</li></ul><h3 id="Core-innovation-Points"><a href="#Core-innovation-Points" class="headerlink" title="Core innovation Points"></a>Core innovation Points</h3><h4 id="Task-1：-Masked-LM"><a href="#Task-1：-Masked-LM" class="headerlink" title="Task #1： Masked LM"></a>Task #1： Masked LM</h4><p>作者认为用单向模型或者把两个单向模型拼接起来不如直接用双向的模型。但是传统方法只能是单向训练。BERT没有使用传统的left-to-right和right-to-left的language model，提出了一种Masked Language Model</p><p>随机屏蔽或替换一些单词，然后去预测这些单词，相当于完形填空…</p><ul><li>80%的概率将单词换为[mask]标记 , my dog is hairy → my dog is [MASK]</li><li>10%的概率将单词换为字表中其他单词 , my dog is hairy → my dog is apple （可能是为了加噪声，防止过拟合） </li><li>10%的概率不替换 , my dog is hairy → my dog is hairy （这个我猜可能是为了让模型学到正常理解方式，而非完型填空的理解方式）</li></ul><h4 id="Task-2：-Next-Sentence-Prediction"><a href="#Task-2：-Next-Sentence-Prediction" class="headerlink" title="Task #2： Next Sentence Prediction"></a>Task #2： Next Sentence Prediction</h4><p>输入一个句子对，来预测第二个句子是不是第一个句子的下一句，学习句子之间的逻辑关系</p><h3 id="Fine-tuning-Procedure"><a href="#Fine-tuning-Procedure" class="headerlink" title="Fine-tuning Procedure"></a>Fine-tuning Procedure</h3><p><img src="/images/bert3.png" alt="image"><br>最后，一般自监督模型的泛化能力是比较强的，BERT从头到尾都是自监督<br>还有模型简直太大了，不是大公司根本训练不起…<br>不过用用预训练词向量还是可以的<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a>   目前支持英文，中文（很难受，只有字），其他的语言（不清楚有哪些）<br>应该是有史以来最好的TPU推广广告吧…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;《BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding》阅读笔记&quot;&gt;&lt;a href=&quot;#《BERT-Pre-training-of-Deep-Bidirect
      
    
    </summary>
    
      <category term="NLP" scheme="https://lizekang.github.io/categories/NLP/"/>
    
    
      <category term="BERT" scheme="https://lizekang.github.io/tags/BERT/"/>
    
      <category term="self attention" scheme="https://lizekang.github.io/tags/self-attention/"/>
    
      <category term="transformer" scheme="https://lizekang.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>《Visual Question Answering with Memory-Augmented Networks》阅读笔记</title>
    <link href="https://lizekang.github.io/2018/11/19/%E3%80%8AVisual-Question-Answering-with-Memory-Augmented-Networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://lizekang.github.io/2018/11/19/《Visual-Question-Answering-with-Memory-Augmented-Networks》阅读笔记/</id>
    <published>2018-11-19T06:53:26.000Z</published>
    <updated>2018-11-20T03:12:11.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="《Visual-Question-Answering-with-Memory-Augmented-Networks》"><a href="#《Visual-Question-Answering-with-Memory-Augmented-Networks》" class="headerlink" title="《Visual Question Answering with Memory-Augmented Networks》"></a>《Visual Question Answering with Memory-Augmented Networks》</h2><blockquote><p>前段时间比赛所需，读了一些关于VQA（Visual Question and Answering）的论文，这篇文章是CVPR2018的一篇文章，亮点在于使用了Memory-Augmented Networks来增加模型对低频答案的识别率。</p></blockquote><ol><li><p><strong>Abstract</strong><br> 在传统的VQA问题中，我们采用梯度下降来更新模型，这种更新方式会使模型预测更倾向于高频答案，本篇论文采用Memory Augmented Network的方法来增加对低频答案的识别率。</p></li><li><p><strong>论文整体框架</strong><br><img src="/images/VQAMAN1.png" alt="image"><br>主要分为三个部分：图片与问题特征提取，图片和问题特征融合（Co-attention），记忆增强网络。</p><ol><li>图片与问题特征提取<br> 这边用的方法比较传统。对问题，通过双向LSTM去提取特征；对于图片，使用VGG等预训练模型提取特征。</li><li>图片与问题特征融合（Co-Attention）<br> 这里借鉴了2016年NIPS一篇论文<a href="https://arxiv.org/pdf/1606.00061.pdf" target="_blank" rel="noopener">《Hierarchical Question-Image Co-Attention for Visual Question Answering》</a><br> <img src="/images/VQAMAN2.png" alt="image"><br> 方法比较简单，直接上公式了<br> <img src="/images/VQAMAN3.png" alt="image"><br> <img src="/images/VQAMAN4.png" alt="image"><br> <img src="/images/VQAMAN5.png" alt="image"></li><li><p>记忆增强网络<br> 这个模块算是这篇文章的亮点吧，在之江杯视频问答中我复现了这个模块，但是效果不是很好:(<br> 在这个模块中用了一个LSTM网络做Controller，以及一个External Memory $M$<br> 在训练过程中，对于所有训练数据${x_{t}, y_{t}}; t= 1…T$,把$x_{t}$丢进controller里面去得到一个query $h_{t}$<br> $$ h_{t} = LSTM(x_{t}, h_{t-1}) $$<br> 之后通过这个query去从Memory中读取信息<br> $$ D(h_{t}, M_{t}(i)) = \frac{h_{t}\cdot M_{t}(i)}{|h_{t}||M_{t}(i)|} $$<br> $$ w^{r}<em>{t} = softmax(D(h</em>{t}, M_{t}(i))) $$<br> $$ r_{t} = \sum_{i} w^{r}<em>{t}(i)M</em>{i} $$</p><blockquote><p>we would like the writer to strike a balance between writing new information to rarely used location and writing to recently used location</p></blockquote><p> 这里，作者提出了一种平衡低频memory和高频memory的读写的方法。<br> $$ w^{u}<em>{t} = \gamma w^{u}</em>{t-1} + w_{t}^{r} + w_{t}^{w} $$<br> $$ w^{w}<em>{t} = (1 - \sigma(\alpha))w^{r}</em>{t-1} + (1-\sigma(\alpha))1(w_{t-1}^{u}\leqslant m(w_{t-1}^{u}, n)) $$<br> $$ M_{t}^{i} = M_{t-1}^{i} + w_{t}^{w}*h_{t} $$<br> 注意：这边的$\alpha$是learnable的，来控制低频和高频的使用频率。<br> 最后把$r_{t}$ 和 $h_{t}$拼接起来丢到分类器里面。</p></li></ol></li><li>Summary</li></ol><ul><li>这篇论文其实看懂了非常好理解，理论上可以一定程度上解决数据分布不平衡导致模型更倾向于预测高频答案的问题。但是这个模型我一直没有调好，不清楚是因为什么问题。</li><li>最近Memory Network很火，主要是因为它模拟人的记忆，使用更少的样本就可以达到相同的效果，也就是Meta-learning的思想，什么One-shot， Zero-shot之类的。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;《Visual-Question-Answering-with-Memory-Augmented-Networks》&quot;&gt;&lt;a href=&quot;#《Visual-Question-Answering-with-Memory-Augmented-Networks》&quot; cl
      
    
    </summary>
    
      <category term="CV" scheme="https://lizekang.github.io/categories/CV/"/>
    
    
      <category term="VideoQA" scheme="https://lizekang.github.io/tags/VideoQA/"/>
    
      <category term="Memory Network" scheme="https://lizekang.github.io/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>《Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering》阅读笔记</title>
    <link href="https://lizekang.github.io/2018/11/19/%E3%80%8ATag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://lizekang.github.io/2018/11/19/《Tag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering》阅读笔记/</id>
    <published>2018-11-19T06:48:00.000Z</published>
    <updated>2018-11-20T03:12:03.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《Tag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering》"><a href="#《Tag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering》" class="headerlink" title="《Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering》"></a>《Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering》</h1><blockquote><p>最近在研究多视角数据增强的方法，读到了这一篇，感觉思想很重要 </p></blockquote><blockquote><p>这篇文章是IJCAI2017的最佳学生论文，简称TD-GAN，用于从单个输入图像中提取可分解的特征，并通过调整所学特征来重新渲染图像。</p></blockquote><blockquote><p>附上链接：<a href="https://www.ijcai.org/proceedings/2017/0404.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2017/0404.pdf</a> </p></blockquote><blockquote><p>目前我复现了一部分，还没写完，<a href="https://github.com/lizekang/TD-GAN" target="_blank" rel="noopener">https://github.com/lizekang/TD-GAN</a></p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h2><p>本文提出了一种全新的神经网络框架，标签分解生成对抗网络(Tag Disentangled Generative Adversarial Networks, TDGAN)，用于进行目标图像的再次渲染(Re-rendering)。给定目标图像作为输入，该网络(TDGAN)即可根据指定要求修改图像内容，并生成符合描述的图像。例如，改变输入图像的观察角度，光照条件，人脸表情等等。和以往工作不同，通过利用图像与其标签的对应关系，即标签是图像分解表征(disentangled representations, DR)的Embedding，我们训练分解网络以提取输入图像的分解表征(DR)。生成网络根据这些表征以及新的标签重新渲染图像。</p><h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a><strong>Network Architecture</strong></h2><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541148671827-ffdb82c2-f8d6-487a-86cf-10f6cf3c490c-image-resized.png" alt="0_1541148666873_ffdb82c2-f8d6-487a-86cf-10f6cf3c490c-image.png"> </p><p>文章中使用了四个网络，a disentangling network, a generative network, a tag mapping net, and a discriminative network。</p><ul><li><p><strong>Disentangling Network</strong></p><p>  用来分解图片特征的网络，将图片的表征分解成几个独立的domain，比如光照，表情，viewpoint等。</p></li><li><p><strong>Tag Mapping Net</strong></p><p>  用来将不同Domain的标签mapping到Disentangling Network分解出来的对应domain上，之后就可以使用Tag Mapping Net生成的特征来替换掉Disentangling Network产生的特征了。</p></li><li><p><strong>Generative Network</strong></p><p>  生成器，根据重组后的特征生成图像</p></li><li><p><strong>Discriminative Network</strong></p><p>  辨别器，辨别生成的图像真/假</p></li></ul><p>看完网络结构感觉这篇文章其实思路很简单，就是替换feature罢了，但是我觉得真正困难的地方是这样一个比较复杂的网络该怎么去训练。果不其然，作者用大篇幅介绍了如何训练以及参数设置，可以说是很良心了。</p><h2 id="网络训练策略"><a href="#网络训练策略" class="headerlink" title="网络训练策略"></a><strong>网络训练策略</strong></h2><p>直接上公式了，对应着图看着公式基本能明白是干什么的（带*的代表冻结该网络）</p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149687254-f4de0d54-b59b-47c2-8f29-b6a274d170b2-image.png" alt="0_1541149682977_f4de0d54-b59b-47c2-8f29-b6a274d170b2-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149697379-231aa761-1d37-4bab-81fc-72bcf7a553d7-image.png" alt="0_1541149693295_231aa761-1d37-4bab-81fc-72bcf7a553d7-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149707361-dd76a590-633c-43bd-956b-e942b30e0e5a-image.png" alt="0_1541149703259_dd76a590-633c-43bd-956b-e942b30e0e5a-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149716175-4894adff-1e9b-4f2d-a7c3-67c8913ba5a2-image.png" alt="0_1541149712084_4894adff-1e9b-4f2d-a7c3-67c8913ba5a2-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149737248-5050c717-05b0-4034-8859-5f32aab07d4d-image.png" alt="0_1541149733132_5050c717-05b0-4034-8859-5f32aab07d4d-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149745460-0f3c071d-e9cf-49f4-8c40-0b59ad8da7ef-image.png" alt="0_1541149741368_0f3c071d-e9cf-49f4-8c40-0b59ad8da7ef-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149754886-00ac8a97-4843-4ece-80fd-0dc3e84f9f9e-image.png" alt="0_1541149750800_00ac8a97-4843-4ece-80fd-0dc3e84f9f9e-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149762921-706d4983-ec97-48e6-8696-f4f0dc1f67b5-image.png" alt="0_1541149758827_706d4983-ec97-48e6-8696-f4f0dc1f67b5-image.png"> </p><h2 id="网络参数设置"><a href="#网络参数设置" class="headerlink" title="网络参数设置"></a><strong>网络参数设置</strong></h2><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149883166-db41c557-2d34-4785-998d-542aa2ce737c-image.png" alt="0_1541149878814_db41c557-2d34-4785-998d-542aa2ce737c-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149892729-14ba550e-772a-445c-8ceb-42dae0d8ff58-image.png" alt="0_1541149888581_14ba550e-772a-445c-8ceb-42dae0d8ff58-image.png"> </p><h2 id="惊艳的实验结果"><a href="#惊艳的实验结果" class="headerlink" title="惊艳的实验结果"></a><strong>惊艳的实验结果</strong></h2><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149948275-8965ff18-c92b-4159-9be3-0c4ea99a3e43-image.png" alt="0_1541149944111_8965ff18-c92b-4159-9be3-0c4ea99a3e43-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149958343-d5d2357b-9f08-48a8-9c50-b1f27cfd6626-image.png" alt="0_1541149954104_d5d2357b-9f08-48a8-9c50-b1f27cfd6626-image.png"> </p><p><img src="https://bbs.dian.org.cn/assets/uploads/files/1541149977353-7e7fc577-811b-49d5-bc14-14ac3fc81533-image.png" alt="0_1541149973197_7e7fc577-811b-49d5-bc14-14ac3fc81533-image.png"></p><h2 id="一点思考"><a href="#一点思考" class="headerlink" title="一点思考"></a><strong>一点思考</strong></h2><p>转自：<a href="http://www.sohu.com/a/257772655_473283" target="_blank" rel="noopener">http://www.sohu.com/a/257772655_473283</a></p><p>多视角学习：面向决策策略的“盲人摸象”</p><p>大家都知道盲人摸象的故事，实际上我们做决策的时候，跟盲人是一样的，因为我们所获取到的信息也是不完整的。那么我们在做觉得时候，也就是根据已有的信息作出的最优策略。因此，对于同样的事情，每一个人所作出的决定可能也不相同。</p><p>多视角学习对于现今的智能系统非常重要，这是因为智能系统中都安装了大量的传感器，比如，现在的无人车安装了激光雷达、毫米波雷达、摄像机、IMU等等。每个传感器都只能够感知环境中的部分信息，那么我们就需要把不同的传感的信息融合起来，帮助我们做最后的决策。</p><p>假设存在一个oracle space，那么每个传感器就可以被建模成对oracle space的一个线性或者非线形投影。如果我们有大量的传感器，那么我们就能够获取大量的投影信息。我们可以证明，如果说我们有足够多的不同的投影信息，我们就能够以非常高的概率去重构这个oracle space。有了这个oracle space，我们就可以有效的做决策了。</p><p>请大家看一下最左边的这张图像。你第一眼看到了什么？大多说人一定会说是船。然后你还会注意到船上有人。对不对？这个现象提示我们，这样的顺序信息对于我们进行多标签学习会非常有帮助。通过增强学习，我们可以有效的学习这个顺序，来提升增强学习的效率。</p><p>我们今天所面临的学习问题可能是这样的一个情况：训练数据和测试数据来自不同的传感器或者信息域。这就是domain generalization要解决的问题。因为训练数据和测试数据来自不同的域，我们就需要找寻一些特征：这些特征在训练数据上和测试数据上，对于完成我们的规定任务来说都是有效的。</p><p>人可以很轻松的做到这一点：我儿子3岁的时候，我给他看过长颈鹿的卡通画片。当我带他去动物园的时候，他可能很轻松的认出真正的长颈鹿。可是在这之前，他从来没有见过实际场景中的长颈鹿。我们当然希望计算机也具备类似的能力。这里我们利用GAN网络（对抗生成网络）能够有效地学习这样的不变特征。</p><p>我们提出了一个端到端的条件对抗域自适应深度学习模型来学习域不变的特征，该模型同时衡量分布P(Y)和条件概率分布P(X|Y)的不变性。该网络框架包括了四个部分。第一部分AlexNet用来学习域不变的特征。第二部分是图像分类网络，用来保证学习的特征具有良好的类别区分性。</p><p>特征的域不变性质利用类别先验归一化域分类网络和类别条件域分类网络保证。其中类别先验归一化域分类网络用来匹配不同域的类别先验归一化分布，该网络的主要目的是消除不同域之间的变化。其次，类别条件域分类网络用来保证对于每一类的分布匹配。这样就能够保证不同域的联合概率分布是匹配的。在不同标准数据集上得到的实验结果证明了我们方法的有效性，并且要比现有方法有显著的提高。</p><p>最近大家开始关注学习的可解释性。我们用GAN网络可以学到特征来生成我们需要的数据。可是这些特征的含义是什么？我们并不清楚。</p><p>通过模仿人类理解世界的方式，我们希望计算机能够从这个复杂的世界中学习到抽象的概念，并根据这些概念创造新的东西。因此，我们需要计算机能够从真实世界图像中提取到可分解的特征，例如照片中人物的身份，拍摄角度，光照条件等等。这个就是tag disentanglement。有了可分解的特征，我们也能很好的解释我们学习到的特征到底是什么物理含义。</p><p>我们提出了一个新的框架（TD-GAN），用于从单个输入图像中提取可分解的特征，并通过调整所学特征来重新渲染图像。从某种程度上说，TD-GAN提供了一个可以理解现实世界中图像的深度学习框架。</p><p>网络所学习到的可分解的特征，实际上对应于图像中所描述主体的不同属性。与人类理解世界的方式相似，学习可分解的特征有助于机器解释并重构现实世界的图像。因此，TD-GAN能够根据用户指定的信息合成高质量的输出图像。</p><p>TD-GAN可应用于（1）数据增强，即通过合成新的图像以用于其他深度学习算法的训练与测试，（2）生成给定对象连续姿态的图像，以用于三维模型重建，以及（3）通过解析，概括来增强现有创作，并创造充满想象力的新绘画。</p><h2 id="多视角数据增强"><a href="#多视角数据增强" class="headerlink" title="多视角数据增强"></a><strong>多视角数据增强</strong></h2><p>我们目前做的多视角图片数据增强和TD-GAN又有一些不同。TD-GAN仅仅是在chairs这个dataset上产生了很好的效果，不过，试想，当一个物体较为复杂的时候，不可能根据一张图片生成这么完美的图像，那我们怎样产生多视角图像呢？</p><p>这恰恰是我们正在研究的内容，我们在探究一种让网络学会融合各视角的图像，如何使网络将各视角的图像组合起来，在“大脑”中构思出这个物体的“3d模型”。这其实是多视角学习，多模态学习很关键的一步。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;《Tag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering》&quot;&gt;&lt;a href=&quot;#《Tag-Disentangled-Generative-Adversarial
      
    
    </summary>
    
      <category term="CV" scheme="https://lizekang.github.io/categories/CV/"/>
    
    
      <category term="GAN" scheme="https://lizekang.github.io/tags/GAN/"/>
    
      <category term="Multi-view Learning" scheme="https://lizekang.github.io/tags/Multi-view-Learning/"/>
    
  </entry>
  
  <entry>
    <title>boosting详解</title>
    <link href="https://lizekang.github.io/2017/11/21/boosting%E8%AF%A6%E8%A7%A3/"/>
    <id>https://lizekang.github.io/2017/11/21/boosting详解/</id>
    <published>2017-11-21T07:30:00.000Z</published>
    <updated>2018-11-20T03:12:23.879Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="Boosting算法简介"><a href="#Boosting算法简介" class="headerlink" title="Boosting算法简介"></a>Boosting算法简介</h1><blockquote><p>李泽康 2017.10.21 AI Lab</p></blockquote><h2 id="PAC"><a href="#PAC" class="headerlink" title="PAC"></a>PAC</h2><ul><li>Probably approximately correct（概率近似正确）</li><li>Strongly learnable（强可学习）<ul><li>Exist an algorithm that requires error can be driven arbitrarily close to 0.​</li></ul></li><li>Weakly learnable（弱可学习）<ul><li>Exist an algorithm that only requires the hypothesis that does better than random guessing.</li></ul></li><li>一个概念是强可学习的充分必要条件是这个概念是弱可学习的。<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2></li><li><p>Adaboost从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成一个强分类器。</p></li><li><p>它是一个加法模型:<br>$$ f(x)=\sum <em>{m=1}^{M}\alpha</em>{m} G_{m}(x)$$<br>$\alpha <em>{m}$ ：分类器的系数， $G</em>{m}(x)$ :不同的弱分类器 </p></li><li><p>损失函数为指数函数：</p><p>$$ (\alpha <em>{m}, G</em>{m}(x)) = \arg \min_{\alpha, G} \sum_{i=1}^{N}\overline w_{mi}exp[-y_{i}\alpha G(x_{i})]$$  </p><p>其中 $$ \overline w_{mi} = exp[-y_{i}f_{m-1}(x_{i})]$$   </p></li><li><p>Adaboost 算法原理(以二分类为例)</p><p>输入： $$ (x_{i}, y_{i}), y={-1, +1}$$</p><ol><li><p>初始化训练数据的权值分布：$$ D_{1} = (w_{11}, …,w_{1i},…,w_{1N})$$ , $$w_{1i} = \frac{1}{N}, i=1,2,…,N$$</p></li><li><p>对$$m=1, 2, …, M$$ :</p><ol><li><p>使用权值分布为$$D_{m}$$ 训练得到弱分类器：$G_{m}(x)$</p></li><li><p>计算误差率：$$e_{m} = \sum <em>{i=1}^{N}w</em>{mi}I(G_{m}(x_{i}) \neq y_{i})$$ </p></li><li><p>计算分类器系数$\alpha _{m}$ : $$\alpha <em>{m} = \frac{1}{2}log\frac{1-e</em>{m}}{e_m}$$ </p></li><li><p>更新权值分布： </p><p>$$D_{m+1} = (w_{m+1,1},…,w_{m+1,i}, … , w_{m+1,N})$$ </p><p>$$w_{m+1, i} = \frac{w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$$ </p><p>$$Z_{m} = \sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$$  </p><p>其中，规范化因子$Z_{m}$ 使$D_{m+1}$ 成为一个概率分布</p></li></ol></li><li><p>构建分类器的线性组合:$$ f(x)=\sum <em>{m=1}^{M}\alpha</em>{m} G_{m}(x)$$ 得到最终分类器$$G_{m}(x) = sign(f(x))$$ </p></li></ol></li><li><p>一些分析</p><ol><li><p>Adaboost的正则化（防止过拟合）：定义learning_rate $l$ ,前面有：$f_{m}(x) = f_{m-1}(x) + \alpha_mG_{m}(x)$ </p><p>加上正则化项有：$f_{m}(x) = f_{m-1}(x) + l\alpha_mG_{m}(x)$ </p></li><li><p>$\alpha_{m}$随$e_{m}$ 减小而增大，分类错误率越小的基本分类器在最终分类器中的作用越大</p></li><li><p>adaboost不改变数据，只是改变数据的权值</p></li><li><p>Adaboost的训练误差界：</p><p>$$\frac{1}{N}\sum_{i=1}^{N}I(G_(x_{i}) \neq y_{i}) \leq \frac{1}{N}\sum_{i}exp(-y_{i}f(x_{i})) = \prod_{m}Z_{m} \leq exp(-2\sum_{m=1}^{M}\gamma_{m}^{2}), \gamma_{m} = \frac{1}{2} - e_{m}$$ </p><p>So, adaboost训练误差以指数速率下降！</p></li><li><p>Adaboost使用比较广泛的弱学习器是决策树和神经网络。</p></li><li><p>优势：分类精度比较高，比较灵活可以使用各种回归、分类模型作为弱学习器，不容易发生过拟合</p><p>缺点：对异常数据比较敏感，可能对异常数据赋较大权值。</p></li></ol></li></ul><h2 id="GDBT"><a href="#GDBT" class="headerlink" title="GDBT"></a>GDBT</h2><ul><li><p>Gradient Decent <strong>Boosting Tree</strong> </p></li><li><p>Gradient Boosting 就是在函数空间的梯度下降</p></li><li><p>是一个加法模型：</p><p>$$F(x;w)=\sum_{m=0}^{M}a_{m}h_{m}(x;w_{m})=\sum_{m=0}^{M}f_{m}(x;w_{m})$$  </p><p>$x$ ：输入样本，$h_{m}$  :分类回归树，w：回归树的参数，$\alpha_{m}$  :每棵树权重</p></li><li><p>GDBT算法原理</p><p>输入：$$(x_{i}, y_{i}), M, L$$ </p><ol><li>初始化$f_{0}$  </li><li>对$m = 1,2,…,M$ :<ol><li>响应：$$\widetilde {y}<em>{i} = -[\frac{\partial L(y</em>{i}, F(x_{i}))}{\partial F(x_{i})}]<em>{F(x)=F</em>{m-1}(x)}, i=1,…,N$$   </li><li>学习第m棵树：$$w_{m} = \arg \min_{w}\sum_{i=1}^{N}(\widetilde y_{i}-h_{m}(x_{i}; w_{m-1}))^{2}$$  </li><li>line search 寻找步长：$$\rho_{m} = \arg \min_{\rho} \sum_{i=1}^{N}L(y_{i}, F_{m-1}(x_{i}) + \rho h(x_{i};w_{m}))  $$  </li><li>令$f_{m} = \rho_{m}h_{m}$ 则$F_{m} = F_{m-1} + f_{m}$  </li></ol></li></ol></li><li><p>一些分析</p><ol><li>GDBT就是使用决策树作为基分类器的boosting方法。与Adaboost不同的是GDBT是一个不断拟合残差并直接叠加到F上的过程，残差不断减小，Loss不断减小。</li><li>在GDBT里，F是泛函空间，f是函数，组合时直接叠加。而Adaboost中f是有权重的，在GDBT中权重信息被吸收到决策树的叶子节点里了。</li><li>初始化的时候<ul><li>随机</li><li>用训练样本的充分统计量初始化</li><li>用其他模型的预测值初始化，例：<a href="http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf" target="_blank" rel="noopener">GDBT在搜索引擎排序中的应用</a> 它使用了RF的输出作为GDBT的初始化，取得了不错的效果。</li></ul></li><li>使用GDBT生成新特征，<a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at Facebook, 2014. </a> </li></ol></li></ul><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><ul><li><p>GDBT是函数空间的梯度下降，XGBoost是函数空间的牛顿法</p></li><li><p>相比于GDBT，XGBoost多了正则化项，目标函数变为：$$\sum_{L}L(\hat y_{i}; y_{i}) + \sum_{k}\Omega(f_{k})$$ </p><p>XGBoost采用的正则化项：$$\Omega(f) = \gamma T + \frac{1}{2}\lambda||w||^{2}$$ , 其中，T：叶子节点数，w：叶节点分数</p></li><li><p>误差函数的二阶泰勒展开：</p><ol><li><p>在第t次迭代后：$$\hat y_{i}^{(t)} = \hat y_{i}^{(t-1)} + f_{t}(x_{i})$$ </p></li><li><p>此时损失函数可以写为：$$L(\hat y_{i}^{(t-1)} + f_{t}(x_{i}), y_{i})$$ ，需要学习的只有$f_{t}$ </p></li><li><p>在$$\hat y_{i}^{(t-1)}$$ 处将损失函数进行二阶泰勒展开：$$L(\hat y_{i}^{(t-1)}, y_{i}) + g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})$$ </p><p>其中，$$g_{i} = \partial_{\hat y^{(t-1)}}L(\hat y^{(t-1)},y_{i}) $$  , $$h_{i} = \partial^{2}<em>{\hat y^{(t-1)}}L(\hat y^{(t-1)}, y</em>{i})$$ </p></li><li><p>显然，$$L(\hat y_{i}^{(t-1)}, y_{i})$$ 为常数，可以消掉，目标函数：$$\widetilde {\mathcal {L}}^{(t)} = \sum_{i=1}^{N}[ g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})] + \Omega(f_{t})$$   </p></li><li><p>$$\widetilde {\mathcal {L}}^{(t)} = \sum_{i=1}^{N}[ g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})] + \gamma T + \frac{1}{2}||w||^{2} = \sum_{i=1}^{N}[g_{i}w_{q_(x_{i})} + \frac{1}{2}h_{i}w_{q(x_{i})}] + \gamma T + \frac{1}{2}\lambda \sum <em>{j=1}^{T} w</em>{j}^{2}$$  </p></li><li><p>这个式子感觉挺复杂，现在我们要把他们统一起来，$w_{q(x_{i})}$ 表示回归树对每个样本的预测值，$w_{j}$ 表示每个叶子节点的分数，$q(x_{i})$ 表示将样本分到某个叶子节点上。我们可以设集合$I_{j} = {i|q(x_{i}) = j}$ ，此时：</p><p>$$\widetilde {\mathcal {L}}^{(t)} = \sum_{j=1}^{T}[\sum_{i\in I_{j}}g_{i}w_{j} + \frac{1}{2} (\sum_{i\in I_{j}}h_{i} + \lambda)w_{j}^{2}] + \gamma T = \sum <em>{j=1}^{T}[G</em>{j}w_{j} + \frac{1}{2} (H_{j} + \lambda )w_{j}^{2}] + \gamma T$$   </p></li><li><p>如何来使损失函数最小化：假定树的结构确定了，那么我们需要调整的只是$w_{j}$ ，当$$\widetilde {\mathcal {L}}^{(t)}$$ 对$w_{j}$ 导数为0时，取到极小，此时可以得到最优预测分数和最小损失：</p><p>$$w_{j}^{<em>} = - \frac{G_{j}}{H_{j} + \lambda}$$ , $$\widetilde {\mathcal {L}}^{</em>} = -\frac{1}{2}\sum_{j=1}^{T}\frac{G_{j}^{2}}{H_{j} + \lambda } + \gamma T$$ </p></li><li><p>So， 怎么去确定树的结构呢？</p><ol><li>暴力解所有树结构（np难啊）</li><li>贪心，每次分裂一个节点，计算增益：$$Gain = \frac{G_{L}^{2}}{H_{L} + \lambda} + \frac{G_{R}^{2}}{H_{R} + \lambda} - \frac{(G_{L} + G_{R})^{2}}{(H_{L} + H_{R}) + \lambda} -\gamma$$ <ul><li>exact greedy</li><li>Weighted Quantile Sketch</li></ul></li></ol></li></ol></li><li><p>并行</p></li><li><p>优势：</p><ol><li>加入了正则化</li><li>使用了二阶导信息</li><li>列抽样来防止过拟合</li><li>Weighted Quantile Sketch</li><li>样本数据预先排序，存储为block，利于并行</li><li>可以对缺失值自动处理</li></ol></li><li><p>调参</p><blockquote><p><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a></p></blockquote></li></ul><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><blockquote><p>一个基于决策树算法的分布式梯度提升框架，<a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/Microsoft/LightGBM</a></p></blockquote><ul><li><p>与xgboost的速度比较</p><p>| <strong>Data</strong>  | <strong>xgboost</strong> | <strong>xgboost_hist</strong> | <strong>LightGBM</strong>     |<br>| ——— | ———– | —————- | —————- |<br>| Higgs     | 3794.34 s   | 551.898 s        | <strong>238.505513 s</strong> |<br>| Yahoo LTR | 674.322 s   | 265.302 s        | <strong>150.18644 s</strong>  |<br>| MS LTR    | 1251.27 s   | 385.201 s        | <strong>215.320316 s</strong> |<br>| Expo      | 1607.35 s   | 588.253 s        | <strong>138.504179 s</strong> |<br>| Allstate  | 2867.22 s   | 1355.71 s        | <strong>348.084475 s</strong> |</p></li><li><p>内存占用比较：</p><p>| <strong>Data</strong>  | <strong>xgboost</strong> | <strong>xgboost_hist</strong> | <strong>LightGBM</strong> |<br>| ——— | ———– | —————- | ———— |<br>| Higgs     | 4.853GB     | 3.784GB          | <strong>0.868GB</strong>  |<br>| Yahoo LTR | 1.907GB     | 1.468GB          | <strong>0.831GB</strong>  |<br>| MS LTR    | 5.469GB     | 3.654GB          | <strong>0.886GB</strong>  |<br>| Expo      | 1.553GB     | 1.393GB          | <strong>0.543GB</strong>  |<br>| Allstate  | 6.237GB     | 4.990GB          | <strong>1.027GB</strong>  |</p></li><li><p>准确率比较：</p><p>| <strong>Data</strong>  | <strong>Metric</strong> | <strong>xgboost</strong> | <strong>xgboost_hist</strong> | <strong>LightGBM</strong> |<br>| :——-: | ———- | ———– | —————- | ———— |<br>|   Higgs   | AUC        | 0.839593    | 0.845605         | 0.845154     |<br>| Yahoo LTR | NDCG1      | 0.719748    | 0.720223         | 0.732466     |<br>|  MS LTR   | NDCG1      | 0.483956    | 0.488649         | 0.524255     |<br>|   Expo    | AUC        | 0.756713    | 0.777777         | 0.777543     |</p></li><li><p>两者区别：</p><ol><li><p>切分算法：</p><ol><li><p>xgboost基于预排序（pre-sorted），对所有特征进行预排序，在寻找分割点时代价为O(#data)。很精确，但空间，时间，cache优化不友好。</p></li><li><p>LightGBM使用了Histogram算法,它主要是把连续的浮点值离散化为k个整数，构造一个宽度为#bins（k）的直方图。根据这些离散值寻找最优分割。内存消耗大大降低，这样不需要存储预排序的结果，而且可以只保存特征离散后的值，这个值一般用8位整型存就够了，而在预排序算法中，需要使用32位分别储存index和feature value。内存消耗降低到1/8。时间复杂度从O(#data <em> #feature)变为O(#bins </em> #feature)。很好的利用了弱模型集成的思想。</p><p><img src="http://www.msra.cn/wp-content/uploads/2017/06/4caedc7agw1fbfnmnhh0qj20fh071q3e.jpg" alt="直方图算法"> </p><p>​</p></li></ol></li><li><p>决策树生长策略：</p><ol><li>xgboost使用level-wise（容易多线程优化，控制模型复杂度，不容易过拟合，但比较低效）</li><li>LightGBM使用带深度限制的leaf-wise</li></ol></li><li><p>LightGBM 的直方图差加速优化，提升一倍速度。</p></li><li><p>高效并行</p><ol><li><p>原始</p><p><strong>特征并行</strong>    的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。</p><p><strong>数据并行</strong>则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。</p></li><li><p>LightGBM针对这两种并行方法都做了优化</p><p>在<strong>特征并行</strong>算法中，通过在<strong>本地保存全部数据</strong>避免对数据切分结果的<strong>通信</strong>；</p><p>在<strong>数据并行</strong>中使用<strong>分散规约(Reduce scatter)</strong>把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用<strong>直方图做差</strong>，进一步减少了一半的通信量。<strong>基于投票的数据并行(Parallel Voting)</strong>则进一步优化数据并行中的通信代价，使通信代价变成常数级别。</p></li></ol></li><li><p>LightGBM是第一个直接支持类别特征的GBDT工具</p></li></ol></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">Greedy function approximation a gradient boosting machine. J.H.Friedman(1999)</a>  </li><li><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System. T. Chen, C. Guestrin (2016)</a> </li><li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiQjuDHmP_WAhUI5yYKHa96CRIQFggmMAA&amp;url=http%3A%2F%2Fwww.jennwv.com%2Fcourses%2FF11%2Flecture13.pdf&amp;usg=AOvVaw1ZToE-UxP82QKandDEP0Di" target="_blank" rel="noopener">CS260 : Lecture 13: Weak vs. Strong Learning and the Adaboost Algorithm</a> </li><li>《统计学习方法》李航著</li><li><a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="noopener">LightGBM(github)</a>   </li><li><a href="http://www.msra.cn/zh-cn/news/features/lightgbm-20170105" target="_blank" rel="noopener">http://www.msra.cn/zh-cn/news/features/lightgbm-20170105</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;Boosting算法简介&quot;&gt;&lt;a href=&quot;#Boosting算法简介&quot; class=&quot;headerlink&quot; title=&quot;Boosting算法简介&quot;&gt;&lt;/a&gt;Boosting算法简介&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;李泽康 
      
    
    </summary>
    
      <category term="ML" scheme="https://lizekang.github.io/categories/ML/"/>
    
    
      <category term="Boosting" scheme="https://lizekang.github.io/tags/Boosting/"/>
    
      <category term="机器学习" scheme="https://lizekang.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="https://lizekang.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://lizekang.github.io/2017/10/01/hello-world/"/>
    <id>https://lizekang.github.io/2017/10/01/hello-world/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-11-19T07:42:04.352Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
