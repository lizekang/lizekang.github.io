<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[年度爆款BERT!]]></title>
    <url>%2F2018%2F11%2F19%2F%E5%B9%B4%E5%BA%A6%E7%88%86%E6%AC%BEBERT%2F</url>
    <content type="text"><![CDATA[《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》阅读笔记BERT算是今年的一个爆款了，同一个模型，只微调就刷新了十一个记录，都有很大的提升…它的出现为之后NLP的发展打下了很好的基础下面来看论文中的做法 Related works论文中介绍了预训练的各种方法: Feature-based Approaches (ELMo) Fine-tuning Approaches (OpenAI GPT， BERT) Transfer Learning from Supervised Data论文中介绍了BERT与其他各种预训练模型ELMo和OpenAI GPT的区别，ELMo使用了双向LSTM提取特征，属于feature-based方法。给downstream的任务生成features。OpenAI GPT只使用了left-to-right的Transformer，而token的right方向的信息是获取不到的。而BERT在所有层中都融合left-to-right和right-to-left的信息。 Input RepresentationsBERT简单粗暴的将句子拼接在一起（无论是单句还是句子对，连在一块就完事了），它的Input Representations 如图 Token embedding 用了WordPiece embedding，这个方法是用来解决oov的word的，就是把一些单词拆成两个词，比如playing就可以拆成play和##ing，拆的时候就用贪心去搜索最少的token来覆盖所有的单词 Segment embedding 区分一下句子一和二 Position embedding 融入了位置信息，是通过模型学到的 The input embeddings is the sum of the token embeddings, the segmentation embeddings and the position embeddings. 没错，就这么粗暴 Core innovation PointsTask #1： Masked LM作者认为用单向模型或者把两个单向模型拼接起来不如直接用双向的模型。但是传统方法只能是单向训练。BERT没有使用传统的left-to-right和right-to-left的language model，提出了一种Masked Language Model 随机屏蔽或替换一些单词，然后去预测这些单词，相当于完形填空… 80%的概率将单词换为[mask]标记 , my dog is hairy → my dog is [MASK] 10%的概率将单词换为字表中其他单词 , my dog is hairy → my dog is apple （可能是为了加噪声，防止过拟合） 10%的概率不替换 , my dog is hairy → my dog is hairy （这个我猜可能是为了让模型学到正常理解方式，而非完型填空的理解方式） Task #2： Next Sentence Prediction输入一个句子对，来预测第二个句子是不是第一个句子的下一句，学习句子之间的逻辑关系 Fine-tuning Procedure最后，一般自监督模型的泛化能力是比较强的，BERT从头到尾都是自监督还有模型简直太大了，不是大公司根本训练不起…不过用用预训练词向量还是可以的https://github.com/google-research/bert 目前支持英文，中文（很难受，只有字），其他的语言（不清楚有哪些）应该是有史以来最好的TPU推广广告吧…]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>self attention</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Visual Question Answering with Memory-Augmented Networks》阅读笔记]]></title>
    <url>%2F2018%2F11%2F19%2F%E3%80%8AVisual-Question-Answering-with-Memory-Augmented-Networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[《Visual Question Answering with Memory-Augmented Networks》 前段时间比赛所需，读了一些关于VQA（Visual Question and Answering）的论文，这篇文章是CVPR2018的一篇文章，亮点在于使用了Memory-Augmented Networks来增加模型对低频答案的识别率。 Abstract 在传统的VQA问题中，我们采用梯度下降来更新模型，这种更新方式会使模型预测更倾向于高频答案，本篇论文采用Memory Augmented Network的方法来增加对低频答案的识别率。 论文整体框架主要分为三个部分：图片与问题特征提取，图片和问题特征融合（Co-attention），记忆增强网络。 图片与问题特征提取 这边用的方法比较传统。对问题，通过双向LSTM去提取特征；对于图片，使用VGG等预训练模型提取特征。 图片与问题特征融合（Co-Attention） 这里借鉴了2016年NIPS一篇论文《Hierarchical Question-Image Co-Attention for Visual Question Answering》 方法比较简单，直接上公式了 记忆增强网络 这个模块算是这篇文章的亮点吧，在之江杯视频问答中我复现了这个模块，但是效果不是很好:( 在这个模块中用了一个LSTM网络做Controller，以及一个External Memory $M$ 在训练过程中，对于所有训练数据${x_{t}, y_{t}}; t= 1…T$,把$x_{t}$丢进controller里面去得到一个query $h_{t}$ $$ h_{t} = LSTM(x_{t}, h_{t-1}) $$ 之后通过这个query去从Memory中读取信息 $$ D(h_{t}, M_{t}(i)) = \frac{h_{t}\cdot M_{t}(i)}{|h_{t}||M_{t}(i)|} $$ $$ w^{r}{t} = softmax(D(h{t}, M_{t}(i))) $$ $$ r_{t} = \sum_{i} w^{r}{t}(i)M{i} $$ we would like the writer to strike a balance between writing new information to rarely used location and writing to recently used location 这里，作者提出了一种平衡低频memory和高频memory的读写的方法。 $$ w^{u}{t} = \gamma w^{u}{t-1} + w_{t}^{r} + w_{t}^{w} $$ $$ w^{w}{t} = (1 - \sigma(\alpha))w^{r}{t-1} + (1-\sigma(\alpha))1(w_{t-1}^{u}\leqslant m(w_{t-1}^{u}, n)) $$ $$ M_{t}^{i} = M_{t-1}^{i} + w_{t}^{w}*h_{t} $$ 注意：这边的$\alpha$是learnable的，来控制低频和高频的使用频率。 最后把$r_{t}$ 和 $h_{t}$拼接起来丢到分类器里面。 Summary 这篇论文其实看懂了非常好理解，理论上可以一定程度上解决数据分布不平衡导致模型更倾向于预测高频答案的问题。但是这个模型我一直没有调好，不清楚是因为什么问题。 最近Memory Network很火，主要是因为它模拟人的记忆，使用更少的样本就可以达到相同的效果，也就是Meta-learning的思想，什么One-shot， Zero-shot之类的。]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>VideoQA</tag>
        <tag>Memory Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering》阅读笔记]]></title>
    <url>%2F2018%2F11%2F19%2F%E3%80%8ATag-Disentangled-Generative-Adversarial-Networks-for-Object-Image-Re-rendering%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[《Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering》 最近在研究多视角数据增强的方法，读到了这一篇，感觉思想很重要 这篇文章是IJCAI2017的最佳学生论文，简称TD-GAN，用于从单个输入图像中提取可分解的特征，并通过调整所学特征来重新渲染图像。 附上链接：https://www.ijcai.org/proceedings/2017/0404.pdf 目前我复现了一部分，还没写完，https://github.com/lizekang/TD-GAN Abstract本文提出了一种全新的神经网络框架，标签分解生成对抗网络(Tag Disentangled Generative Adversarial Networks, TDGAN)，用于进行目标图像的再次渲染(Re-rendering)。给定目标图像作为输入，该网络(TDGAN)即可根据指定要求修改图像内容，并生成符合描述的图像。例如，改变输入图像的观察角度，光照条件，人脸表情等等。和以往工作不同，通过利用图像与其标签的对应关系，即标签是图像分解表征(disentangled representations, DR)的Embedding，我们训练分解网络以提取输入图像的分解表征(DR)。生成网络根据这些表征以及新的标签重新渲染图像。 Network Architecture 文章中使用了四个网络，a disentangling network, a generative network, a tag mapping net, and a discriminative network。 Disentangling Network 用来分解图片特征的网络，将图片的表征分解成几个独立的domain，比如光照，表情，viewpoint等。 Tag Mapping Net 用来将不同Domain的标签mapping到Disentangling Network分解出来的对应domain上，之后就可以使用Tag Mapping Net生成的特征来替换掉Disentangling Network产生的特征了。 Generative Network 生成器，根据重组后的特征生成图像 Discriminative Network 辨别器，辨别生成的图像真/假 看完网络结构感觉这篇文章其实思路很简单，就是替换feature罢了，但是我觉得真正困难的地方是这样一个比较复杂的网络该怎么去训练。果不其然，作者用大篇幅介绍了如何训练以及参数设置，可以说是很良心了。 网络训练策略直接上公式了，对应着图看着公式基本能明白是干什么的（带*的代表冻结该网络） 网络参数设置 惊艳的实验结果 一点思考转自：http://www.sohu.com/a/257772655_473283 多视角学习：面向决策策略的“盲人摸象” 大家都知道盲人摸象的故事，实际上我们做决策的时候，跟盲人是一样的，因为我们所获取到的信息也是不完整的。那么我们在做觉得时候，也就是根据已有的信息作出的最优策略。因此，对于同样的事情，每一个人所作出的决定可能也不相同。 多视角学习对于现今的智能系统非常重要，这是因为智能系统中都安装了大量的传感器，比如，现在的无人车安装了激光雷达、毫米波雷达、摄像机、IMU等等。每个传感器都只能够感知环境中的部分信息，那么我们就需要把不同的传感的信息融合起来，帮助我们做最后的决策。 假设存在一个oracle space，那么每个传感器就可以被建模成对oracle space的一个线性或者非线形投影。如果我们有大量的传感器，那么我们就能够获取大量的投影信息。我们可以证明，如果说我们有足够多的不同的投影信息，我们就能够以非常高的概率去重构这个oracle space。有了这个oracle space，我们就可以有效的做决策了。 请大家看一下最左边的这张图像。你第一眼看到了什么？大多说人一定会说是船。然后你还会注意到船上有人。对不对？这个现象提示我们，这样的顺序信息对于我们进行多标签学习会非常有帮助。通过增强学习，我们可以有效的学习这个顺序，来提升增强学习的效率。 我们今天所面临的学习问题可能是这样的一个情况：训练数据和测试数据来自不同的传感器或者信息域。这就是domain generalization要解决的问题。因为训练数据和测试数据来自不同的域，我们就需要找寻一些特征：这些特征在训练数据上和测试数据上，对于完成我们的规定任务来说都是有效的。 人可以很轻松的做到这一点：我儿子3岁的时候，我给他看过长颈鹿的卡通画片。当我带他去动物园的时候，他可能很轻松的认出真正的长颈鹿。可是在这之前，他从来没有见过实际场景中的长颈鹿。我们当然希望计算机也具备类似的能力。这里我们利用GAN网络（对抗生成网络）能够有效地学习这样的不变特征。 我们提出了一个端到端的条件对抗域自适应深度学习模型来学习域不变的特征，该模型同时衡量分布P(Y)和条件概率分布P(X|Y)的不变性。该网络框架包括了四个部分。第一部分AlexNet用来学习域不变的特征。第二部分是图像分类网络，用来保证学习的特征具有良好的类别区分性。 特征的域不变性质利用类别先验归一化域分类网络和类别条件域分类网络保证。其中类别先验归一化域分类网络用来匹配不同域的类别先验归一化分布，该网络的主要目的是消除不同域之间的变化。其次，类别条件域分类网络用来保证对于每一类的分布匹配。这样就能够保证不同域的联合概率分布是匹配的。在不同标准数据集上得到的实验结果证明了我们方法的有效性，并且要比现有方法有显著的提高。 最近大家开始关注学习的可解释性。我们用GAN网络可以学到特征来生成我们需要的数据。可是这些特征的含义是什么？我们并不清楚。 通过模仿人类理解世界的方式，我们希望计算机能够从这个复杂的世界中学习到抽象的概念，并根据这些概念创造新的东西。因此，我们需要计算机能够从真实世界图像中提取到可分解的特征，例如照片中人物的身份，拍摄角度，光照条件等等。这个就是tag disentanglement。有了可分解的特征，我们也能很好的解释我们学习到的特征到底是什么物理含义。 我们提出了一个新的框架（TD-GAN），用于从单个输入图像中提取可分解的特征，并通过调整所学特征来重新渲染图像。从某种程度上说，TD-GAN提供了一个可以理解现实世界中图像的深度学习框架。 网络所学习到的可分解的特征，实际上对应于图像中所描述主体的不同属性。与人类理解世界的方式相似，学习可分解的特征有助于机器解释并重构现实世界的图像。因此，TD-GAN能够根据用户指定的信息合成高质量的输出图像。 TD-GAN可应用于（1）数据增强，即通过合成新的图像以用于其他深度学习算法的训练与测试，（2）生成给定对象连续姿态的图像，以用于三维模型重建，以及（3）通过解析，概括来增强现有创作，并创造充满想象力的新绘画。 多视角数据增强我们目前做的多视角图片数据增强和TD-GAN又有一些不同。TD-GAN仅仅是在chairs这个dataset上产生了很好的效果，不过，试想，当一个物体较为复杂的时候，不可能根据一张图片生成这么完美的图像，那我们怎样产生多视角图像呢？ 这恰恰是我们正在研究的内容，我们在探究一种让网络学会融合各视角的图像，如何使网络将各视角的图像组合起来，在“大脑”中构思出这个物体的“3d模型”。这其实是多视角学习，多模态学习很关键的一步。]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>Multi-view Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[boosting详解]]></title>
    <url>%2F2017%2F11%2F21%2Fboosting%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[TOC] Boosting算法简介 李泽康 2017.10.21 AI Lab PAC Probably approximately correct（概率近似正确） Strongly learnable（强可学习） Exist an algorithm that requires error can be driven arbitrarily close to 0.​ Weakly learnable（弱可学习） Exist an algorithm that only requires the hypothesis that does better than random guessing. 一个概念是强可学习的充分必要条件是这个概念是弱可学习的。Adaboost Adaboost从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成一个强分类器。 它是一个加法模型:$$ f(x)=\sum {m=1}^{M}\alpha{m} G_{m}(x)$$$\alpha {m}$ ：分类器的系数， $G{m}(x)$ :不同的弱分类器 损失函数为指数函数： $$ (\alpha {m}, G{m}(x)) = \arg \min_{\alpha, G} \sum_{i=1}^{N}\overline w_{mi}exp[-y_{i}\alpha G(x_{i})]$$ 其中 $$ \overline w_{mi} = exp[-y_{i}f_{m-1}(x_{i})]$$ Adaboost 算法原理(以二分类为例) 输入： $$ (x_{i}, y_{i}), y={-1, +1}$$ 初始化训练数据的权值分布：$$ D_{1} = (w_{11}, …,w_{1i},…,w_{1N})$$ , $$w_{1i} = \frac{1}{N}, i=1,2,…,N$$ 对$$m=1, 2, …, M$$ : 使用权值分布为$$D_{m}$$ 训练得到弱分类器：$G_{m}(x)$ 计算误差率：$$e_{m} = \sum {i=1}^{N}w{mi}I(G_{m}(x_{i}) \neq y_{i})$$ 计算分类器系数$\alpha _{m}$ : $$\alpha {m} = \frac{1}{2}log\frac{1-e{m}}{e_m}$$ 更新权值分布： $$D_{m+1} = (w_{m+1,1},…,w_{m+1,i}, … , w_{m+1,N})$$ $$w_{m+1, i} = \frac{w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$$ $$Z_{m} = \sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))$$ 其中，规范化因子$Z_{m}$ 使$D_{m+1}$ 成为一个概率分布 构建分类器的线性组合:$$ f(x)=\sum {m=1}^{M}\alpha{m} G_{m}(x)$$ 得到最终分类器$$G_{m}(x) = sign(f(x))$$ 一些分析 Adaboost的正则化（防止过拟合）：定义learning_rate $l$ ,前面有：$f_{m}(x) = f_{m-1}(x) + \alpha_mG_{m}(x)$ 加上正则化项有：$f_{m}(x) = f_{m-1}(x) + l\alpha_mG_{m}(x)$ $\alpha_{m}$随$e_{m}$ 减小而增大，分类错误率越小的基本分类器在最终分类器中的作用越大 adaboost不改变数据，只是改变数据的权值 Adaboost的训练误差界： $$\frac{1}{N}\sum_{i=1}^{N}I(G_(x_{i}) \neq y_{i}) \leq \frac{1}{N}\sum_{i}exp(-y_{i}f(x_{i})) = \prod_{m}Z_{m} \leq exp(-2\sum_{m=1}^{M}\gamma_{m}^{2}), \gamma_{m} = \frac{1}{2} - e_{m}$$ So, adaboost训练误差以指数速率下降！ Adaboost使用比较广泛的弱学习器是决策树和神经网络。 优势：分类精度比较高，比较灵活可以使用各种回归、分类模型作为弱学习器，不容易发生过拟合 缺点：对异常数据比较敏感，可能对异常数据赋较大权值。 GDBT Gradient Decent Boosting Tree Gradient Boosting 就是在函数空间的梯度下降 是一个加法模型： $$F(x;w)=\sum_{m=0}^{M}a_{m}h_{m}(x;w_{m})=\sum_{m=0}^{M}f_{m}(x;w_{m})$$ $x$ ：输入样本，$h_{m}$ :分类回归树，w：回归树的参数，$\alpha_{m}$ :每棵树权重 GDBT算法原理 输入：$$(x_{i}, y_{i}), M, L$$ 初始化$f_{0}$ 对$m = 1,2,…,M$ : 响应：$$\widetilde {y}{i} = -[\frac{\partial L(y{i}, F(x_{i}))}{\partial F(x_{i})}]{F(x)=F{m-1}(x)}, i=1,…,N$$ 学习第m棵树：$$w_{m} = \arg \min_{w}\sum_{i=1}^{N}(\widetilde y_{i}-h_{m}(x_{i}; w_{m-1}))^{2}$$ line search 寻找步长：$$\rho_{m} = \arg \min_{\rho} \sum_{i=1}^{N}L(y_{i}, F_{m-1}(x_{i}) + \rho h(x_{i};w_{m})) $$ 令$f_{m} = \rho_{m}h_{m}$ 则$F_{m} = F_{m-1} + f_{m}$ 一些分析 GDBT就是使用决策树作为基分类器的boosting方法。与Adaboost不同的是GDBT是一个不断拟合残差并直接叠加到F上的过程，残差不断减小，Loss不断减小。 在GDBT里，F是泛函空间，f是函数，组合时直接叠加。而Adaboost中f是有权重的，在GDBT中权重信息被吸收到决策树的叶子节点里了。 初始化的时候 随机 用训练样本的充分统计量初始化 用其他模型的预测值初始化，例：GDBT在搜索引擎排序中的应用 它使用了RF的输出作为GDBT的初始化，取得了不错的效果。 使用GDBT生成新特征，Practical Lessons from Predicting Clicks on Ads at Facebook, 2014. XGBoost GDBT是函数空间的梯度下降，XGBoost是函数空间的牛顿法 相比于GDBT，XGBoost多了正则化项，目标函数变为：$$\sum_{L}L(\hat y_{i}; y_{i}) + \sum_{k}\Omega(f_{k})$$ XGBoost采用的正则化项：$$\Omega(f) = \gamma T + \frac{1}{2}\lambda||w||^{2}$$ , 其中，T：叶子节点数，w：叶节点分数 误差函数的二阶泰勒展开： 在第t次迭代后：$$\hat y_{i}^{(t)} = \hat y_{i}^{(t-1)} + f_{t}(x_{i})$$ 此时损失函数可以写为：$$L(\hat y_{i}^{(t-1)} + f_{t}(x_{i}), y_{i})$$ ，需要学习的只有$f_{t}$ 在$$\hat y_{i}^{(t-1)}$$ 处将损失函数进行二阶泰勒展开：$$L(\hat y_{i}^{(t-1)}, y_{i}) + g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})$$ 其中，$$g_{i} = \partial_{\hat y^{(t-1)}}L(\hat y^{(t-1)},y_{i}) $$ , $$h_{i} = \partial^{2}{\hat y^{(t-1)}}L(\hat y^{(t-1)}, y{i})$$ 显然，$$L(\hat y_{i}^{(t-1)}, y_{i})$$ 为常数，可以消掉，目标函数：$$\widetilde {\mathcal {L}}^{(t)} = \sum_{i=1}^{N}[ g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})] + \Omega(f_{t})$$ $$\widetilde {\mathcal {L}}^{(t)} = \sum_{i=1}^{N}[ g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i})] + \gamma T + \frac{1}{2}||w||^{2} = \sum_{i=1}^{N}[g_{i}w_{q_(x_{i})} + \frac{1}{2}h_{i}w_{q(x_{i})}] + \gamma T + \frac{1}{2}\lambda \sum {j=1}^{T} w{j}^{2}$$ 这个式子感觉挺复杂，现在我们要把他们统一起来，$w_{q(x_{i})}$ 表示回归树对每个样本的预测值，$w_{j}$ 表示每个叶子节点的分数，$q(x_{i})$ 表示将样本分到某个叶子节点上。我们可以设集合$I_{j} = {i|q(x_{i}) = j}$ ，此时： $$\widetilde {\mathcal {L}}^{(t)} = \sum_{j=1}^{T}[\sum_{i\in I_{j}}g_{i}w_{j} + \frac{1}{2} (\sum_{i\in I_{j}}h_{i} + \lambda)w_{j}^{2}] + \gamma T = \sum {j=1}^{T}[G{j}w_{j} + \frac{1}{2} (H_{j} + \lambda )w_{j}^{2}] + \gamma T$$ 如何来使损失函数最小化：假定树的结构确定了，那么我们需要调整的只是$w_{j}$ ，当$$\widetilde {\mathcal {L}}^{(t)}$$ 对$w_{j}$ 导数为0时，取到极小，此时可以得到最优预测分数和最小损失： $$w_{j}^{} = - \frac{G_{j}}{H_{j} + \lambda}$$ , $$\widetilde {\mathcal {L}}^{} = -\frac{1}{2}\sum_{j=1}^{T}\frac{G_{j}^{2}}{H_{j} + \lambda } + \gamma T$$ So， 怎么去确定树的结构呢？ 暴力解所有树结构（np难啊） 贪心，每次分裂一个节点，计算增益：$$Gain = \frac{G_{L}^{2}}{H_{L} + \lambda} + \frac{G_{R}^{2}}{H_{R} + \lambda} - \frac{(G_{L} + G_{R})^{2}}{(H_{L} + H_{R}) + \lambda} -\gamma$$ exact greedy Weighted Quantile Sketch 并行 优势： 加入了正则化 使用了二阶导信息 列抽样来防止过拟合 Weighted Quantile Sketch 样本数据预先排序，存储为block，利于并行 可以对缺失值自动处理 调参 https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ LightGBM 一个基于决策树算法的分布式梯度提升框架，https://github.com/Microsoft/LightGBM 与xgboost的速度比较 | Data | xgboost | xgboost_hist | LightGBM || ——— | ———– | —————- | —————- || Higgs | 3794.34 s | 551.898 s | 238.505513 s || Yahoo LTR | 674.322 s | 265.302 s | 150.18644 s || MS LTR | 1251.27 s | 385.201 s | 215.320316 s || Expo | 1607.35 s | 588.253 s | 138.504179 s || Allstate | 2867.22 s | 1355.71 s | 348.084475 s | 内存占用比较： | Data | xgboost | xgboost_hist | LightGBM || ——— | ———– | —————- | ———— || Higgs | 4.853GB | 3.784GB | 0.868GB || Yahoo LTR | 1.907GB | 1.468GB | 0.831GB || MS LTR | 5.469GB | 3.654GB | 0.886GB || Expo | 1.553GB | 1.393GB | 0.543GB || Allstate | 6.237GB | 4.990GB | 1.027GB | 准确率比较： | Data | Metric | xgboost | xgboost_hist | LightGBM || :——-: | ———- | ———– | —————- | ———— || Higgs | AUC | 0.839593 | 0.845605 | 0.845154 || Yahoo LTR | NDCG1 | 0.719748 | 0.720223 | 0.732466 || MS LTR | NDCG1 | 0.483956 | 0.488649 | 0.524255 || Expo | AUC | 0.756713 | 0.777777 | 0.777543 | 两者区别： 切分算法： xgboost基于预排序（pre-sorted），对所有特征进行预排序，在寻找分割点时代价为O(#data)。很精确，但空间，时间，cache优化不友好。 LightGBM使用了Histogram算法,它主要是把连续的浮点值离散化为k个整数，构造一个宽度为#bins（k）的直方图。根据这些离散值寻找最优分割。内存消耗大大降低，这样不需要存储预排序的结果，而且可以只保存特征离散后的值，这个值一般用8位整型存就够了，而在预排序算法中，需要使用32位分别储存index和feature value。内存消耗降低到1/8。时间复杂度从O(#data #feature)变为O(#bins #feature)。很好的利用了弱模型集成的思想。 ​ 决策树生长策略： xgboost使用level-wise（容易多线程优化，控制模型复杂度，不容易过拟合，但比较低效） LightGBM使用带深度限制的leaf-wise LightGBM 的直方图差加速优化，提升一倍速度。 高效并行 原始 特征并行 的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。 LightGBM针对这两种并行方法都做了优化 在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信； 在数据并行中使用分散规约(Reduce scatter)把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行(Parallel Voting)则进一步优化数据并行中的通信代价，使通信代价变成常数级别。 LightGBM是第一个直接支持类别特征的GBDT工具 Reference Greedy function approximation a gradient boosting machine. J.H.Friedman(1999) XGBoost: A Scalable Tree Boosting System. T. Chen, C. Guestrin (2016) CS260 : Lecture 13: Weak vs. Strong Learning and the Adaboost Algorithm 《统计学习方法》李航著 LightGBM(github) http://www.msra.cn/zh-cn/news/features/lightgbm-20170105]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Boosting</tag>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F10%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
